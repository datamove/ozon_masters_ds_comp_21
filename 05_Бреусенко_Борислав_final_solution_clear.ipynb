{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Краткое описание решения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Общий план\n",
    "Я выделил несколько ключевых идей, на которые опирался при решениии:  \n",
    "1. **Сделать локальную валидацию, которая будет коррелировать с лидербордом.**  \n",
    "Это даст неограниченное количество проверок своих решений. И какую-то информацию о данных с теста: если обычные методы валидации дают качественный результат, то в тесте нет каких-то подвохов от организаторов.  \n",
    "Финальное разбиение: кросс-валидация для отбора фич (70%) + валидация для проверки решений (20%) + отложенная выборка для финального скора (10%)\n",
    "2. **Провести Adversarial Validation.**  \n",
    "Это также может показать наличие подводных камней в данных. Дополнительно я проверю отсутствие дата ликов в фичах, которые я мог бы упустить.\n",
    "3. **Написать качественные фичи.**   \n",
    "На это опирается мое решение. Я не нашел каких-то ликов в анализе данных, поэтому решил подумать, что было бы наиболее информативным для модели. \n",
    "4. **Работать с градиентным бустингом.**\n",
    "Эту модель я регулярно использую в работе, поэтому хорошо с ней знаком. Дополнительный плюс - он умеет работать с пропущенными значениями.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Признаки\n",
    "Мои топ фичи основаны на знании рейтинга игрока в следующей игре. Я пришел к этому следующим путём:  \n",
    "При анализе данных на трейне и на тесте я заметил, что нет какого-то определенного фактора, который мог бы разделить эти выборки. То есть разбиение данных происходило случайно. При этом мы знаем рейтинги игроков. А значит, можно попробовать написать алгоритм, который по фиче X21 (время игры - game_time) находил бы рейтинг каждого игрока на момент следующей игры. Тогда мы могли бы сделать однозначный вывод о том, кто победил, а кто проиграл.  \n",
    "Но реальность оказалась более сложной. Сложности с которыми я столкнулся:  \n",
    "1. Некоторые игроки проводили несколько игр в один game_time. При этом в каких-то он побеждал, а в каких-то проигрывал. *Решение: усреднять рейтинг игрока по значению game_time.*    \n",
    "2. Для некоторых игроков во всем датасете всего одна запись. *Решение: считать, что признак принимает значение nan в такие моменты.*  \n",
    "3. Основная проблема - победа не всегда означала повышение рейтинга. *Решение: отдать это на откуп бустингу :) Подумал, что стоит сначала посмотреть на результаты прогнозов. Они оказались достаточно хороши, и на этом я остановился.*  \n",
    "4. Я брал информацию по всему датасету. Так делать не совсем правильно в общем случае. Потому что мы используем ту информацию, которой в реальной жизни знать бы не могли. *Решение: в описании и чате было разрешено использовать всё, поэтому я проигнорировал свои сомнения. Тем более, что проверка на дата лики показала, что всё ок*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Остальные фичи\n",
    "Предварительно я написал несколько других фич, которые не имеют большого смысла, но могли бы либо выявить лики в данных (напр. разница в id игроков), либо как-то неявно агрегировать информацию, которую я в ходе анализа данных не нашел (напр. средний номер юнита). В итоге график важности фичей по метрике gain показывает, что основной вклад вносят фичи, связанные с рейтингом в следующей игре. Поэтому нет смысла останавливаться на этом подробно.  \n",
    "\n",
    "Идеи, которые я не реализовал:\n",
    "- отбор фич. Уверен, что на одних фичах по рейтингу можно получить результат не хуже. Но решил, что можно не тратить время на это.  \n",
    "- фичи, связанные с процентом побед игрока. Тут у меня получился даталик, так как на тесте скорее всего есть игроки, которых не было на трейне. И там фича давала бы лик. Не стал этим заниматься, потому что были другие идеи, которые я хотел проверить.  \n",
    "- target encoding. Никогда не применял эту технику, но всегда хотел. Уверен, что при правильной реализации это могло бы дать очень хороший результат в данной задаче. Но я посчитал важным сфокусироваться на других аспектах.  \n",
    "- составление характеристик каждого юнита. Ещё одна идея, которая кажется перспективной, но требует слишком много времени на анализ. Думаю, что если бы контест шел ещё неделю, то я бы её реализовал. К тому же на это дан намёк в описании задания.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Финал\n",
    "Таким был мой общий ход решения. Спасибо большое за организацию соревнования. Было приятно применить свои навыки в новой для себя задаче. Наверняка я упустил какие-то очевидные моменты в данных, поэтому буду рад узнать об этом в каких-то дополнительных материалах о задаче.  \n",
    "Мои результаты:  \n",
    "- 0.565142 -- кросс-валидация  \n",
    "- 0.56496 -- валидация  \n",
    "- 0.56541 -- отложенная выборка  \n",
    "- 0.56521 и пятое место -- приватный лидерборд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm.auto import tqdm\n",
    "import lightgbm as lgb\n",
    "from copy import deepcopy as dp\n",
    "from collections import defaultdict\n",
    "\n",
    "# pandas setting\n",
    "pd.options.display.max_columns = 50\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X: pd.DataFrame, y: pd.DataFrame, cv_frac: float = 0.7, valid_frac: float = 0.2, test_frac: float = 0.1) -> list:\n",
    "    '''\n",
    "    Splits data into three parts + stratification.\n",
    "    Output:\n",
    "    tuple : Three datasets and targets\n",
    "    '''\n",
    "    assert round(cv_frac + valid_frac + test_frac, 5) == 1, f'{cv_frac + valid_frac + test_frac}'\n",
    "    X_cv, X_oof, y_cv, y_oof = train_test_split(X, y, train_size=cv_frac, \n",
    "                                                shuffle=True, random_state=1, stratify=y)\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(X_oof, y_oof, train_size=(valid_frac / (valid_frac + test_frac)), \n",
    "                                                        shuffle=True, random_state=1, stratify=y_oof)\n",
    "    return X_cv, y_cv, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(fitted_model, X_test: pd.DataFrame, y_test: pd.Series) -> list:\n",
    "    '''\n",
    "    Generate predict and calc score.\n",
    "    Output:\n",
    "    tuple : predictions, score\n",
    "    '''\n",
    "    preds = fitted_model.predict_proba(X_test)\n",
    "    score = log_loss(y_test, preds)\n",
    "    score = round(score, 5)\n",
    "    return preds, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series, model, model_params: dict = {}):\n",
    "    '''\n",
    "    Initialization + fitting + results on train and test.\n",
    "    Output:\n",
    "    dict : model, preds on train and test, target, scorec on train and test\n",
    "    '''\n",
    "    curr_model = model(**model_params)\n",
    "    curr_model.fit(X_train, y_train)\n",
    "    \n",
    "    preds_train, train_score = predict(curr_model, X_train, y_train)\n",
    "    preds_test, test_score = predict(curr_model, X_test, y_test)\n",
    "    \n",
    "    return {'model': curr_model, 'preds': preds_test, 'y_true': y_test, 'test_score': test_score, 'train_score': train_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_val_res(X: pd.DataFrame, y: pd.Series, model, model_params: dict, n_splits:int = 5) -> list:\n",
    "    '''\n",
    "    Perform cross-validation.\n",
    "    Output:\n",
    "    list : model, preds on train and test, target, scorec on train and test\n",
    "    '''\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    res = []\n",
    "    for fold_num, (train_index, test_index) in tqdm(enumerate(skf.split(X, y)), total=n_splits):\n",
    "        print('Current fold:', fold_num)\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        output = fit_model(X_train, y_train, X_test, y_test, model, params)\n",
    "        res.append(output)\n",
    "        print(output['train_score'], output['test_score'], end='\\n\\n')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_val_score(res: list, score_col: str = 'test_score') -> float:\n",
    "    '''\n",
    "    Calc score for CV.\n",
    "    Output:\n",
    "    float : mean score over all folds\n",
    "    '''\n",
    "    scores = np.mean([i[score_col] for i in res])\n",
    "    return scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../raw_data/'\n",
    "train_file = 'train.csv'\n",
    "test_file = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{data_folder}/{train_file}')\n",
    "test_df = pd.read_csv(f'{data_folder}/{test_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns for better understandig\n",
    "new_col_names = ['id', 'fight_type', 'p1', 'p1_rating', 'p2', 'p2_rating', \n",
    "                    'p1_u1', 'p1_u2', 'p1_u3', 'p1_u4', 'p1_u5', 'p1_u6', 'p1_u7', 'p1_u8', \n",
    "                    'p2_u1', 'p2_u2', 'p2_u3', 'p2_u4', 'p2_u5', 'p2_u6', 'p2_u7', 'p2_u8', \n",
    "                    'game_time', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns = new_col_names\n",
    "test_df.columns = new_col_names[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check nan values\n",
    "train_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for future features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_info = []\n",
    "for df in [train_df, test_df]:\n",
    "    player_info_1 = df[['p1', 'p1_rating', 'game_time', 'fight_type']].copy()\n",
    "    if 'target' in df:\n",
    "        player_info_1['is_win'] = df['target'].apply(lambda x: x == 1)\n",
    "    else:\n",
    "        player_info_1['is_win'] = np.nan\n",
    "    player_info_2 = df[['p2', 'p2_rating', 'game_time', 'fight_type']].copy()\n",
    "    if 'target' in df:\n",
    "        player_info_2['is_win'] = df['target'].apply(lambda x: x == 0)\n",
    "    else:\n",
    "        player_info_2['is_win'] = np.nan\n",
    "    player_info_1.columns = ['p_id', 'rating', 'game_time', 'fight_type', 'is_win']\n",
    "    player_info_2.columns = ['p_id', 'rating', 'game_time', 'fight_type', 'is_win']\n",
    "    player_info_full = pd.concat([player_info_1, player_info_2], axis=0, ignore_index=True)\n",
    "    players_info.append(dp(player_info_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_info_full = pd.concat(players_info, axis=0, ignore_index=True).sort_values(by='game_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "players_info_full[players_info_full['p_id'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate info about each player\n",
    "player_info_dict = defaultdict(dict)\n",
    "for player_id, rating, game_time, fight_type, is_win in tqdm(players_info_full.to_numpy()):\n",
    "    player_id, rating, game_time, fight_type = int(player_id), int(rating), int(game_time), int(fight_type)\n",
    "    if player_id not in player_info_dict:\n",
    "        player_info_dict[player_id]['game_time'] = []\n",
    "        player_info_dict[player_id]['rating'] = []\n",
    "        player_info_dict[player_id]['fight_type'] = []\n",
    "        player_info_dict[player_id]['is_win'] = []\n",
    "    player_info_dict[player_id]['game_time'].append(game_time)\n",
    "    player_info_dict[player_id]['rating'].append(rating)\n",
    "    player_info_dict[player_id]['fight_type'].append(fight_type)\n",
    "    player_info_dict[player_id]['is_win'].append(is_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "player_info_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ETA ~5 mins, sorry\n",
    "# aggregate info about each game and calc next game rating \n",
    "for key, info in tqdm(player_info_dict.items(), total=len(player_info_dict)):\n",
    "    player_info_dict[key]['win_rate'] = np.nanmean(info['is_win'])\n",
    "    player_info_dict[key]['avg_rating'] = np.nanmean(info['rating'])\n",
    "    player_info_dict[key]['min_rating'] = np.min(info['rating'])\n",
    "    player_info_dict[key]['max_rating'] = np.max(info['rating'])\n",
    "    \n",
    "    game_times = info['game_time']\n",
    "    ratings = info['rating']\n",
    "    \n",
    "    avg_rating = defaultdict(list)  # {game_id: [all ratings for game_id]}\n",
    "    for index in range(len(game_times)):\n",
    "        game_id = game_times[index]\n",
    "        rating = ratings[index]\n",
    "        avg_rating[game_id].append(rating)\n",
    "        \n",
    "        \n",
    "    next_game_ratings = {}\n",
    "    for index in range(len(avg_rating)):\n",
    "        if index + 1 == len(avg_rating):  \n",
    "            curr_game = list((avg_rating.keys()))[index]\n",
    "            next_game_rating = np.nan  # don't have better ideas\n",
    "            next_game_ratings[curr_game] = next_game_rating\n",
    "        else:\n",
    "            curr_game = list((avg_rating.keys()))[index]\n",
    "            next_game = list((avg_rating.keys()))[index + 1]\n",
    "            next_game_rating = np.nanmean(avg_rating[next_game])\n",
    "            next_game_ratings[curr_game] = next_game_rating\n",
    "    \n",
    "    \n",
    "    player_info_dict[key]['next_game_ratings'] = next_game_ratings  # {game_id: next_game_id_rating}   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samle\n",
    "player_info_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=['target'])\n",
    "y = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv, y_cv, X_valid, y_valid, X_test, y_test = split_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv.shape,  X_valid.shape,  X_test.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cv.shape, y_valid.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureMaker():\n",
    "    '''\n",
    "    Calc all features.\n",
    "    Input:\n",
    "    ----------\n",
    "    init_data : pd.DataFrame\n",
    "        Raw data: train or test\n",
    "    Output:\n",
    "    pd.DataFrame : DataFrame with all features \n",
    "    '''\n",
    "    def calc_base_features(self, init_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''Basic information'''\n",
    "        new_features = pd.get_dummies(init_data['fight_type'], prefix='fight_type')\n",
    "        new_features['game_time'] = init_data['game_time']\n",
    "        \n",
    "        return new_features\n",
    "    \n",
    "    def calc_unit_features(self, init_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''Unit-related features'''\n",
    "        p1_units = ['p1_u1', 'p1_u2', 'p1_u3', 'p1_u4', 'p1_u5', 'p1_u6', 'p1_u7', 'p1_u8']\n",
    "        p2_units = ['p2_u1', 'p2_u2', 'p2_u3', 'p2_u4', 'p2_u5', 'p2_u6', 'p2_u7', 'p2_u8']\n",
    "        all_unit_columns = p1_units + p2_units\n",
    "        \n",
    "        new_features = pd.DataFrame(index=init_data.index)\n",
    "        \n",
    "        # trash-features\n",
    "        new_features['p1_mean_unit'] = init_data[p1_units].mean(axis=1)\n",
    "        new_features['p2_mean_unit'] = init_data[p2_units].mean(axis=1)\n",
    "        new_features['p1_median_unit'] = init_data[p1_units].median(axis=1)\n",
    "        new_features['p2_median_unit'] = init_data[p2_units].median(axis=1)\n",
    "        new_features['p1_std_unit'] = init_data[p1_units].std(axis=1)\n",
    "        new_features['p2_std_unit'] = init_data[p2_units].std(axis=1)\n",
    "        new_features['p1_max_unit'] = init_data[p1_units].max(axis=1)\n",
    "        new_features['p2_max_unit'] = init_data[p2_units].max(axis=1)\n",
    "        new_features['p1_min_unit'] = init_data[p1_units].min(axis=1)\n",
    "        new_features['p2_min_unit'] = init_data[p2_units].min(axis=1)\n",
    "        \n",
    "        new_features['mean_unit_diff'] = new_features['p1_mean_unit'] - new_features['p2_mean_unit']\n",
    "        new_features['mean_unit_ratio'] = new_features['p1_mean_unit'] / new_features['p2_mean_unit']\n",
    "        \n",
    "        new_features['std_unit_diff'] = new_features['p1_std_unit'] - new_features['p2_std_unit']\n",
    "        new_features['std_unit_ratio'] = new_features['p1_std_unit'] / new_features['p2_std_unit']\n",
    "        \n",
    "        new_features['max_unit_diff'] = new_features['p1_max_unit'] - new_features['p2_max_unit']\n",
    "        new_features['max_unit_ratio'] = (new_features['p1_max_unit'] + 1) / (new_features['p2_max_unit'] + 1)\n",
    "        \n",
    "        new_features['min_unit_diff'] = new_features['p1_min_unit'] - new_features['p2_min_unit']\n",
    "        new_features['min_unit_ratio'] = (new_features['p1_min_unit'] + 1) / (new_features['p2_min_unit'] + 1)\n",
    "                    \n",
    "        return new_features\n",
    "    \n",
    "    def calc_player_features(self, init_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''Player-related features'''\n",
    "        new_features = pd.DataFrame(index=init_data.index)\n",
    "        # trash features \n",
    "        new_features['id_diff'] = init_data['p1'] - init_data['p2']\n",
    "        return new_features\n",
    "    \n",
    "    def calc_rating_features(self, init_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''Rating features. Here we are going to use player_info_dict data'''\n",
    "        new_features = pd.DataFrame(index=init_data.index)\n",
    "        new_features['rating_diff'] = init_data['p1_rating'] - init_data['p2_rating']\n",
    "        new_features['rating_diff'] = new_features['rating_diff'].apply(lambda x: max(min(-42, x), 42))\n",
    "        \n",
    "        \n",
    "        # player_info_dict features:\n",
    "        for player in ['p1', 'p2']:\n",
    "            new_features[f'{player}_avg_rating'] = init_data[player].apply(lambda x: player_info_dict[x]['avg_rating'])\n",
    "            new_features[f'{player}_min_rating'] = init_data[player].apply(lambda x: player_info_dict[x]['min_rating'])\n",
    "            new_features[f'{player}_max_rating'] = init_data[player].apply(lambda x: player_info_dict[x]['max_rating'])\n",
    "            new_features[f'{player}_rating_after_game'] = init_data[[player, 'game_time']].apply(lambda x: player_info_dict[x[player]]['next_game_ratings'][x['game_time']], axis=1)\n",
    "            new_features[f'{player}_rating_diff'] = new_features[f'{player}_rating_after_game'] - init_data[f'{player}_rating']\n",
    "        new_features['avg_rating_diff'] = new_features['p1_avg_rating'] - new_features['p2_avg_rating']\n",
    "        new_features['min_rating_diff'] = new_features['p1_min_rating'] - new_features['p2_min_rating']\n",
    "        new_features['max_rating_diff'] = new_features['p1_max_rating'] - new_features['p2_max_rating']\n",
    "        new_features['rating_after_game_diff'] = new_features['p1_rating_after_game'] - new_features['p2_rating_after_game']\n",
    "        new_features['rating_diff_diff'] = new_features['p1_rating_diff'] - new_features['p2_rating_diff']\n",
    "        \n",
    "        return new_features\n",
    "    \n",
    "    def fit_transform(self, data: pd.DataFrame) -> pd.DataFrame or list:\n",
    "        base_features = self.calc_base_features(data)\n",
    "        unit_features = self.calc_unit_features(data)\n",
    "        player_features = self.calc_player_features(data)\n",
    "        rating_features = self.calc_rating_features(data)\n",
    "        \n",
    "        new_features = pd.concat([base_features, unit_features, player_features, rating_features], axis=1)\n",
    "#         assert new_features.isna().sum().sum() == 0\n",
    "        assert new_features.shape[0] == data.shape[0]\n",
    "        \n",
    "        return new_features\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = FeatureMaker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_cv_features = fm.fit_transform(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_features = fm.fit_transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_features = fm.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform CV (You can skip this part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier\n",
    "params = {\n",
    "    \"boosting_type\": \"dart\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 6,\n",
    "    \"num_leaves\" : 40,\n",
    "    \"drop_rate\": 0.7,\n",
    "    \"skip_drop\": 0.7,\n",
    "    \"max_drop\": 1,\n",
    "    \"verbosity\": -1,\n",
    "    \"seed\": 42,\n",
    "    \"n_jobs\": 10,\n",
    "    \"n_estimators\": 100\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_cross_val_res(X_cv_features, y_cv, model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cross_val_score(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = res[0]['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(fitted_model, importance_type='gain', figsize=(10, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get valid score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = fit_model(X_cv_features, y_cv, X_valid_features, y_valid, model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, score = predict(output['model'], X_test_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make final predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prod_features = fm.fit_transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data leak test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([X_cv_features, X_prod_features], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.concat([pd.Series(1, index=X_cv.index), pd.Series(0, index=X_prod_features.index)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_cross_val_res(full_df, target, model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(res[0]['y_true'], res[0]['preds'][:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no leaks if features so far!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = output['model'].predict_proba(X_prod_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_file = 'sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(data_folder + submit_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.shape[0] == preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['target'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = '../submissions/'\n",
    "save_name = 'more_rating_features.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(save_folder + save_name, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
